services:
  triton_server:
    image: nvcr.io/nvidia/tritonserver:25.05-py3
    container_name: triton_server
    runtime: nvidia
    shm_size: 256m
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    ports:
      - "8000:8000"  # HTTP endpoint
      - "8001:8001"  # gRPC endpoint
      - "8002:8002"  # Metrics endpoint
    volumes:
      - ./models:/models  # Mount the models directory
    command: [ "tritonserver", "--model-repository=/models" ]